/**
 * Test script to verify the new CLIP model loads correctly
 */

import { CLIPTextModelWithProjection, CLIPVisionModelWithProjection, AutoTokenizer, AutoProcessor } from '@xenova/transformers';

async function testModel() {
  try {
    console.log('ğŸ§ª Testing Xenova/clip-vit-base-patch16 model loading...');
    
    // Test loading the text model
    console.log('ğŸ“ Loading text model...');
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');
    const textModel = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');
    console.log('âœ… Text model loaded successfully');
    
    // Test loading the vision model
    console.log('ğŸ–¼ï¸ Loading vision model...');
    const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');
    const visionModel = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');
    console.log('âœ… Vision model loaded successfully');
    
    // Test text encoding
    console.log('ğŸ“ Testing text encoding...');
    const texts = ['a photo of a cat'];
    const textInputs = tokenizer(texts, { padding: true, truncation: true });
    const { text_embeds } = await textModel(textInputs);
    console.log('ğŸ“ Text embedding shape:', text_embeds.dims);
    console.log('ğŸ“Š Text embedding dimension:', text_embeds.dims[1]);
    
    console.log('âœ… All tests passed! Model is working correctly.');
    
  } catch (error) {
    console.error('âŒ Test failed:', error);
  }
}

// Run the test
testModel();
